# Prompt — 2026-02-14: Multi-Provider AI & Optional Azure Source

## Summary

This session introduced **pluggable AI model providers** and made **every feature optional** except the database. The server starts with only a store connection; chat, embedding, and Azure Graph features are lazily initialized and throw descriptive errors when accessed without configuration.

---

## Changes Overview

### 1. Environment Configuration Overhaul

**File:** `src/config/env.ts`

**Before:** All Azure OpenAI and Microsoft Graph credentials were required at startup. The app crashed immediately if any were missing.

**After:** Only the store credentials (`MONGODB_URI`/`MONGODB_DB_NAME` or `POSTGRES_URL`) are required. Everything else is optional:

| Variable | Required | Default | Purpose |
|---|---|---|---|
| `STORE_TYPE` | Yes | `mongo` | `mongo` or `postgres` |
| `MONGODB_URI` | When STORE_TYPE=mongo | — | MongoDB connection string |
| `MONGODB_DB_NAME` | When STORE_TYPE=mongo | — | Database name |
| `POSTGRES_URL` | When STORE_TYPE=postgres | — | PostgreSQL connection string |
| `EMBED_MODEL_PROVIDER` | No | (unset) | `AZURE`, `OPENAI`, `DEEPSEEK`, `OLLAMA`, `CUSTOM` |
| `EMBED_API_KEY` | Per provider | — | API key for embedding provider |
| `EMBED_MODEL_NAME` | Per provider | — | Model or deployment name |
| `EMBED_BASE_URL` | Per provider | — | Base URL (defaults exist per provider) |
| `EMBED_DIMENSIONS` | No | `1536` | Vector dimensions (must match model output) |
| `CHAT_MODEL_PROVIDER` | No | (unset) | Same provider options as embed |
| `CHAT_API_KEY` | Per provider | — | API key for chat provider |
| `CHAT_MODEL_NAME` | Per provider | — | Model or deployment name |
| `CHAT_BASE_URL` | Per provider | — | Base URL |
| `AZURE_OPENAI_API_VERSION` | When provider=AZURE | `2024-02-01` | Azure API version |
| `AZURE_SOURCE` | No | `N` | Set `Y` to enable MS Graph features |
| `AZURE_TENANT_ID` | When AZURE_SOURCE=Y | — | Microsoft Entra tenant ID |
| `AZURE_CLIENT_ID` | When AZURE_SOURCE=Y | — | App registration client ID |
| `AZURE_CLIENT_SECRET` | When AZURE_SOURCE=Y | — | App registration secret |
| `PORT` | No | `3000` | Server port |
| `LOG_LEVEL` | No | `info` | Pino log level |

**Removed env vars:** `AZURE_OPENAI_API_KEY`, `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT`, `AZURE_OPENAI_CHAT_DEPLOYMENT`, `DISABLE_AZURE_SOURCE`. These are replaced by the generic `CHAT_*`/`EMBED_*` vars.

**Key design:** Chat and embed have **completely separate** config blocks. You can use AZURE for embeddings and OLLAMA for chat, or configure only one and leave the other disabled.

---

### 2. Model Provider Enum

**New file:** `src/enums/model_provider.enum.ts`

```typescript
export const ModelProvider = {
    AZURE: 'AZURE',
    OPENAI: 'OPENAI',
    DEEPSEEK: 'DEEPSEEK',
    OLLAMA: 'OLLAMA',
    CUSTOM: 'CUSTOM',
} as const;
```

Exported from `src/enums/index.ts`.

---

### 3. Chat Model Implementations

All chat models implement the `ChatModel` interface (`src/ai/chat/chat_model.interface.ts`):

```typescript
export interface ChatModel {
    invoke(input: string, context?: string): Promise<string>;
}
```

| File | Class | Providers | SDK |
|---|---|---|---|
| `src/ai/chat/azure.chat_model.ts` | `AzureChatModel` | AZURE | `AzureChatOpenAI` from `@langchain/openai` |
| `src/ai/chat/openai_compat.chat_model.ts` | `OpenAICompatChatModel` | OPENAI, DEEPSEEK, CUSTOM | `ChatOpenAI` from `@langchain/openai` |
| `src/ai/chat/ollama.chat_model.ts` | `OllamaChatModel` | OLLAMA | `ChatOllama` from `@langchain/ollama` |

**System prompt** extracted to `src/ai/chat/system_prompt.ts` — shared by all implementations.

**Default base URLs:**
- OPENAI: `api.openai.com` (langchain default)
- DEEPSEEK: `https://api.deepseek.com`
- OLLAMA: `http://localhost:11434`
- CUSTOM: must be set via `CHAT_BASE_URL`
- AZURE: N/A (uses `CHAT_BASE_URL` as the Azure endpoint)

**Deleted:** `src/ai/chat/openai.chat_model.ts` (was the old Azure-only `OpenAIChatModel` class).

---

### 4. Embedder Implementations

All embedders implement the `Embedder` interface (`src/ai/embed/embedder.interface.ts`):

```typescript
export interface Embedder {
    readonly dimensions: number;
    embed(text: string): Promise<number[]>;
    embed_batch(texts: string[]): Promise<number[][]>;
}
```

| File | Class | Providers | SDK |
|---|---|---|---|
| `src/ai/embed/azure.embedder.ts` | `AzureEmbedder` | AZURE | `AzureOpenAIEmbeddings` from `@langchain/openai` |
| `src/ai/embed/openai_compat.embedder.ts` | `OpenAICompatEmbedder` | OPENAI, DEEPSEEK, CUSTOM | `OpenAIEmbeddings` from `@langchain/openai` |
| `src/ai/embed/ollama.embedder.ts` | `OllamaEmbedder` | OLLAMA | `OllamaEmbeddings` from `@langchain/ollama` |

All embedders read `EMBED_DIMENSIONS` from env to set `this.dimensions`.

**Deleted:** `src/ai/embed/openai.embedder.ts` (was the old Azure-only `OpenAIEmbedder` class).

---

### 5. AI Factory Functions (Provider Selection)

**Files:** `src/ai/chat/index.ts`, `src/ai/embed/index.ts`

Both follow the same pattern:
1. Read the provider env var (`CHAT_MODEL_PROVIDER` / `EMBED_MODEL_PROVIDER`)
2. If empty → throw `ChatNotConfiguredError` / `EmbedNotConfiguredError`
3. `switch` on provider → lazy-`require()` the implementation → instantiate singleton
4. Unknown provider → throw `ProviderUnsupportedError`

Implementations are loaded via `require()` (not top-level `import`) so the specific provider's SDK is only loaded when actually used. This means `@langchain/ollama` is never loaded if you're using OpenAI, etc.

**`src/ai/index.ts`** (unchanged) provides the `ai` namespace with lazy getters:
```typescript
export const ai = {
    get embed(): Embedder { return get_embedder(); },
    get chat(): ChatModel  { return get_chat_model(); },
};
```

---

### 6. Error Classes for AI

**New file:** `src/errors/ai.error.ts`

| Class | Code | Status | When thrown |
|---|---|---|---|
| `ChatNotConfiguredError` | `CHAT_NOT_CONFIGURED` | 503 | `ai.chat` accessed without `CHAT_MODEL_PROVIDER` |
| `EmbedNotConfiguredError` | `EMBED_NOT_CONFIGURED` | 503 | `ai.embed` accessed without `EMBED_MODEL_PROVIDER` |
| `ProviderUnsupportedError` | `PROVIDER_UNSUPPORTED` | 400 | Unknown value in `*_MODEL_PROVIDER` |
| `AzureSourceDisabledError` | `AZURE_SOURCE_DISABLED` | 503 | Graph API feature accessed without `AZURE_SOURCE=Y` |

All extend `AppError` and are handled by the `onError` handler in `src/index.ts`.

Exported from `src/errors/index.ts`.

---

### 7. Azure Source Gating (`AZURE_SOURCE`)

**Concept:** By default, Microsoft Graph API features (Teams transcript ingestion, distribution list ingestion) are **disabled**. Set `AZURE_SOURCE=Y` to enable them, which then requires `AZURE_TENANT_ID`, `AZURE_CLIENT_ID`, `AZURE_CLIENT_SECRET`.

**Affected files:**

| File | Behavior when AZURE_SOURCE ≠ Y |
|---|---|
| `src/config/env.ts` | Graph credentials are optional (not validated) |
| `src/extractors/index.ts` | Transcript/DL extractors are replaced with `DisabledExtractor` stubs that throw `AzureSourceDisabledError` |
| `src/cron/index.ts` | `register_all()` skips transcript/DL cron jobs entirely |
| `src/api/trigger.api.ts` | `/api/trigger/transcripts` and `/api/trigger/dls` throw `AzureSourceDisabledError` (503) |

**PDF features are always available** — they don't use Microsoft Graph.

**Implementation detail:** When Azure source is disabled, the `TranscriptExtractor` and `DLExtractor` classes are **never imported** (lazy `require()`). This avoids pulling in `@microsoft/microsoft-graph-client` and `@azure/identity` at all.

---

### 8. Startup Flow (`src/index.ts`)

The application entry point now handles optional features gracefully:

```
1. Connect to store (always — required)
2. IF EMBED_MODEL_PROVIDER is set:
     → instantiate embedder, get dimensions, run store.setup(dimensions)
   ELSE:
     → log warning: "embedding/RAG features are unavailable"
3. IF CHAT_MODEL_PROVIDER is not set:
     → log warning: "chat features are unavailable"
4. Mount all API routes (always — they throw if accessed without config)
5. Register cron jobs (skips Azure-dependent jobs if AZURE_SOURCE ≠ Y)
6. Start scheduler
7. Register graceful shutdown
```

---

### 9. Dependencies

**Added:** `@langchain/ollama` (^1.2.2) — provides `ChatOllama` and `OllamaEmbeddings`.

**Existing (unchanged):** `@langchain/openai` covers both Azure (`AzureChatOpenAI`, `AzureOpenAIEmbeddings`) and vanilla OpenAI (`ChatOpenAI`, `OpenAIEmbeddings`). DeepSeek and CUSTOM providers use the same OpenAI-compatible classes with a custom `baseURL`.

---

### 10. Updated Reference Files

- **`.env.example`** — Complete rewrite with all new env vars, grouped by feature, with examples for each provider (Azure, OpenAI, DeepSeek, Ollama, Custom).
- **`src/connection/pg/postgres_queries.sql`** — Added note about dynamic dimensions, provider dimension reference, and clarified this is a reference script (the app auto-creates tables at startup).

---

## Current Project Structure

```
src/
├── ai/
│   ├── index.ts                              # ai namespace (lazy embed/chat getters)
│   ├── chat/
│   │   ├── chat_model.interface.ts           # ChatModel interface
│   │   ├── system_prompt.ts                  # Shared RAG system prompt
│   │   ├── index.ts                          # Factory: provider → singleton
│   │   ├── azure.chat_model.ts               # AzureChatModel
│   │   ├── openai_compat.chat_model.ts       # OpenAICompatChatModel (OPENAI/DEEPSEEK/CUSTOM)
│   │   └── ollama.chat_model.ts              # OllamaChatModel
│   └── embed/
│       ├── embedder.interface.ts             # Embedder interface
│       ├── index.ts                          # Factory: provider → singleton
│       ├── azure.embedder.ts                 # AzureEmbedder
│       ├── openai_compat.embedder.ts         # OpenAICompatEmbedder (OPENAI/DEEPSEEK/CUSTOM)
│       └── ollama.embedder.ts                # OllamaEmbedder
├── api/
│   ├── chat.api.ts                           # POST /api/chat
│   ├── pdf.api.ts                            # POST /api/pdf/url, /upload, GET /:id
│   └── trigger.api.ts                        # POST /api/trigger/transcripts, /dls (guarded)
├── config/
│   └── env.ts                                # Typed env config with optional providers
├── connection/
│   ├── microsoft/
│   │   └── graph.client.ts                   # MS Graph client (lazy, only when AZURE_SOURCE=Y)
│   ├── mongo/
│   │   └── mongo.connection.ts
│   └── pg/
│       ├── pg.connection.ts
│       └── postgres_queries.sql              # Reference SQL schema
├── cron/
│   ├── cron.interface.ts                     # CronJobDef interface
│   ├── index.ts                              # cron namespace (register_all guards Azure)
│   ├── scheduler.ts                          # register_job, start_scheduler
│   ├── transcript.cron.ts                    # TranscriptCronJob
│   └── dl.cron.ts                            # DLCronJob
├── enums/
│   ├── index.ts                              # Barrel exports
│   ├── collections.enum.ts                   # Collections
│   ├── file_status.enum.ts                   # FileStatus
│   ├── jobs.enum.ts                          # JobName
│   ├── model_provider.enum.ts                # ModelProvider (AZURE/OPENAI/DEEPSEEK/OLLAMA/CUSTOM)
│   └── source_type.enum.ts                   # SourceType
├── errors/
│   ├── index.ts                              # Barrel exports
│   ├── app.error.ts                          # Base AppError
│   ├── ai.error.ts                           # ChatNotConfigured, EmbedNotConfigured, ProviderUnsupported, AzureSourceDisabled
│   ├── config.error.ts                       # EnvMissingError
│   ├── parse.error.ts                        # ParserNotSupportedError
│   ├── pdf.error.ts                          # PDFOptionsRequired, PDFSourceMissing, PDFDownloadError
│   └── store.error.ts                        # StoreNotConnectedError
├── extractors/
│   ├── extractor.interface.ts                # Extractor<T> interface
│   ├── index.ts                              # extractors namespace (DisabledExtractor stubs when no Azure)
│   ├── microsoft/
│   │   ├── transcript.extractor.ts           # TranscriptExtractor
│   │   └── dl.extractor.ts                   # DLExtractor
│   └── pdf/
│       └── pdf.extractor.ts                  # PDFExtractor
├── mappers/
│   ├── index.ts
│   └── microsoft/
│       ├── transcript.mapper.ts              # mapper$transcript
│       └── dl.mapper.ts                      # mapper$dl
├── parsers/
│   ├── index.ts
│   ├── parser.interface.ts                   # Parser<T> interface
│   ├── transcript.parser.ts                  # TranscriptParser
│   ├── dl.parser.ts                          # DLParser
│   ├── pdf.parser.ts                         # PDFParser / parse_pdf()
│   └── text_chunker.ts                       # chunk_text()
├── services/
│   ├── base.service.ts                       # create_entity_service<T> (generic CRUD)
│   ├── chat.service.ts                       # chat() — RAG query pipeline
│   ├── document.service.ts                   # documents.add/search/delete (VectorStore)
│   ├── file.service.ts                       # files — extends base + update_status
│   ├── sync_state.service.ts                 # sync_state — extends base + get_last_sync
│   └── ingestion/
│       ├── index.ts                          # ingestion namespace
│       ├── transcript.ingest.ts
│       ├── dl.ingest.ts
│       └── pdf.ingest.ts
├── store/
│   ├── index.ts                              # store namespace + factory
│   ├── store.interface.ts                    # Store, VectorStore, MetaStore<T>
│   ├── mongo/
│   │   ├── mongo.store.ts
│   │   ├── mongo.vector_store.ts
│   │   └── mongo.meta_store.ts
│   └── pg/
│       ├── pg.store.ts
│       ├── pg.vector_store.ts
│       └── pg.meta_store.ts
├── types/
│   ├── index.ts
│   ├── chat.types.ts
│   ├── document.types.ts
│   ├── file.types.ts
│   ├── ingest.types.ts
│   ├── microsoft.types.ts
│   ├── search.types.ts
│   └── sync.types.ts
├── utils/
│   └── log.util.ts                           # Pino logger
└── index.ts                                  # Entry point
```

---

## Example Configurations

### Minimal (store only — no AI)
```env
STORE_TYPE=mongo
MONGODB_URI=mongodb+srv://...
MONGODB_DB_NAME=knowledge_base
PORT=3000
```
Server starts. All AI/Graph endpoints return 503 with descriptive messages.

### Embeddings only (for testing RAG pipeline)
```env
STORE_TYPE=mongo
MONGODB_URI=mongodb+srv://...
MONGODB_DB_NAME=knowledge_base
EMBED_MODEL_PROVIDER=OPENAI
EMBED_API_KEY=sk-...
EMBED_MODEL_NAME=text-embedding-3-small
```
PDF ingestion + vector search work. Chat endpoint returns 503.

### Full stack (Azure everything)
```env
STORE_TYPE=mongo
MONGODB_URI=mongodb+srv://...
MONGODB_DB_NAME=knowledge_base
EMBED_MODEL_PROVIDER=AZURE
EMBED_API_KEY=<key>
EMBED_BASE_URL=https://myinstance.openai.azure.com
EMBED_MODEL_NAME=text-embedding-3-small
CHAT_MODEL_PROVIDER=AZURE
CHAT_API_KEY=<key>
CHAT_BASE_URL=https://myinstance.openai.azure.com
CHAT_MODEL_NAME=gpt-4o
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_SOURCE=Y
AZURE_TENANT_ID=...
AZURE_CLIENT_ID=...
AZURE_CLIENT_SECRET=...
```
Everything works including transcript/DL ingestion and cron jobs.

### Mixed providers
```env
STORE_TYPE=postgres
POSTGRES_URL=postgresql://...
EMBED_MODEL_PROVIDER=OLLAMA
EMBED_MODEL_NAME=nomic-embed-text
EMBED_BASE_URL=http://localhost:11434
EMBED_DIMENSIONS=768
CHAT_MODEL_PROVIDER=DEEPSEEK
CHAT_API_KEY=sk-...
CHAT_MODEL_NAME=deepseek-chat
```
Local Ollama for embeddings, DeepSeek cloud for chat, PostgreSQL store. No Azure Graph features.

---

## Error Behavior Matrix

| Feature accessed | Missing config | Error thrown | HTTP status |
|---|---|---|---|
| `ai.embed.*` | `EMBED_MODEL_PROVIDER` not set | `EmbedNotConfiguredError` | 503 |
| `ai.chat.*` | `CHAT_MODEL_PROVIDER` not set | `ChatNotConfiguredError` | 503 |
| `ai.embed.*` / `ai.chat.*` | Unknown provider value | `ProviderUnsupportedError` | 400 |
| `/api/trigger/transcripts` | `AZURE_SOURCE` ≠ Y | `AzureSourceDisabledError` | 503 |
| `/api/trigger/dls` | `AZURE_SOURCE` ≠ Y | `AzureSourceDisabledError` | 503 |
| `/api/chat` | No embed provider | `EmbedNotConfiguredError` | 503 |
| `/api/chat` | No chat provider | `ChatNotConfiguredError` | 503 |
| `/api/pdf/*` | No embed provider | `EmbedNotConfiguredError` | 503 |
| Store operations | DB not connected | `StoreNotConnectedError` | 500 |
| Any env var missing | `required()` call | `EnvMissingError` | 500 |

---

## How to Add a New Provider

1. Create `src/ai/chat/<name>.chat_model.ts` implementing `ChatModel`
2. Create `src/ai/embed/<name>.embedder.ts` implementing `Embedder` (if it supports embeddings)
3. Add the provider name to `ModelProvider` enum in `src/enums/model_provider.enum.ts`
4. Add a `case` in `src/ai/chat/index.ts` and/or `src/ai/embed/index.ts`
5. Install the SDK package (e.g., `bun add @langchain/anthropic`)
6. Document the required env vars in `.env.example`
